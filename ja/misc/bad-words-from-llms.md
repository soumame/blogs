---
title: LLMが人に悪い行動を促すことについて
emoji: 🤖
description: LLMが自殺幇助をしていると主張する人がいるということを知り、いろいろな人が当たり前のようにLLMという得体の知れないブラックボックスを使うようになった今、どう自分はLLMと向き合えばいいのか考えてみる
tags:
  - essay
  - brain-storming
  - llm
published_at: 2025-12-22
isTranslated:
---
こんな記事を見た

https://gigazine.net/news/20251108-seven-families-suing-openai-chatgpt-suicides/

人間の言動を学習しているLLMは、その性質上人に悪いとされる行動を促してしまうケースがある。LLMの回答がきっかけで、自殺を試みてしまったり、ユーザーが死亡してしまったりすることが起きているそう。で、OpenAIが訴えられている。

## 日本の状況
まあこれはアメリカの話で、訴訟社会だからこういった訴訟が複数起きているのかもしれないけど、日本でも起こり得ると思っている。

実際日本は自殺者が少なくないし、日本でユーザー数が多いソーシャルメディアであるXとかでそういった行動をほのめかす投稿などをしている人を自分は見かけたことがある

自殺のきっかけは様々だと思う。昔、自分は「お前なんか死んじゃえばいいのに」とか、そういう安直な言い方がきっかけになると思っていたけど、どうやらそんなに単純ではないみたい。

https://www.mhlw.go.jp/content/001464717.pdf

いじめだけでなく、仕事の過労、生活困窮、孤立などもその理由として挙げられている。

こういった理由で、生きているのが辛いと感じる人がいたとして、そういった人が何らかのきっかけで自殺を試みてしまうのは想像できる。

## AIは責任を取るのか
そうすると、今までそのきっかけが、誰かネット上の人の言動だったり、自殺のニュースだったりしていたものが、LLMによる発言がきっかけになることも普通にありえそう。

でも、人と違うのは、彼らは人間のやっていることを真似してくれるおもちゃで、企業が機械の上で動かして運営している。

そして、その機械で動かして発言した内容に責任を取ることはできない。世界中のあらゆるデータを食って育っているので、作った人達は、どんなデータを食わせたのか、全て把握しているわけないし、データを食べた結果、何が出てくるかは、それが出てくるまでわからない。
要するに、ブラックボックスということになる。すでに1人の人間が理解できる範囲を超えていると思う。

でも、ブラックボックスという点では人間も似ていると思っていて、人間は、他の人間が発する言葉を学んで、適切なタイミングでそれを出力することができる。当たり前だけど。

さらに、どんなデータを食ったのかがわからないところも同じだ。どんな環境で、どんなコミュニティで、どんな背景があってその発言があるのか...ある程度は本人が覚えているかもしれないけれど、大半は覚えていないと思う。ちなみに自分は最初に家の前に来ているゴミ収集車を指刺して、「ゴーシーシー」と言ったらしい。もちろんそれは合っている。見ているものを言葉にしたので、VLMと同じだ。でもそれがなぜゴミ収集車だったのか、誰から学んだのかは誰も知らない。父親かもしれないし、母親かもしれないし、絵本かもしれない。

データのソースが不明確だし、明確にしようとしてもここまでデータが溢れている世界でそれをやろうとするのはかなり困難だと思う。

そんな何を学んで育ったかわからないやつ（LLM）に人間は信頼を置いていて、今やどこにでもいる。

責任の所在について考えると、また難しくて、企業とかだったら、企業の偉い「人」が、責任を取ると言えば済む話だ。自分は忙しいから、代わりにAIにできることをやってもらう。でも、その責任は私が取るし、何か間違ったことをしたら保証すると言えば、顧客は信用してくれるだろう。場合によっては人間以上のパフォーマンスを出すこともある。

でも、それが単なる話し相手だとどうなるか？私たちは友達に責任を持たせるのか？「あなたの言ったことが私に悪影響を与えたら保証しろ」なんて友達に言ったら縁切られるに決まってる。企業と顧客の関係みたいに、商品に対価を払うわけでもない。もちろん有益な情報を共有したり、学んだり、楽しんだり、するけれど、対価は払わないし、ほとんどの場合（結婚など、法的拘束力がある場合を除き）責任を取ることはない。

LLMの位置づけにもよるけど、ChatGPT, Gemini, Claudeみたいなチャット形式で文章を提供するサービスで、そういった責任を取る仕組みを設けることはかなり難しいと思う。特定の分野（例えば、プログラミングとか）で正確な回答を返します！とかだったらまだできるかもだけど、あまりにも分野が広すぎる。もちろん、不適切な発言をしないようにある程度の制御はできる（事前にそういったデータを食わせないようにしたり、あるいはそういった出力が出たときに、表示しないようにする）けど、万が一そういった回答が出てしまったとき、責任を取るなんて言ってしまったらキリがない。なんなら不適切に見えてなくても、その人にとっては不適切かもしれない。

### じゃあどうするか
なので、もう免責事項として書いておいて、利用時に許諾してもらうしかない気がすると思っている。

「あなたに友達を貸すよ。私たちは厳しく、丹精込めてその子を育てたの。悪いことは学ばないようにさせたよ。だから、多分変なこと言わないと思うけど、私たちが知らないこととか、あなたにとって良くない影響を与えてしまうことを知っているかもしれない。それでもあなたはこの人と話すのね？」と聞くしかないのだ。改めて、人に例えると、変な感じだね。

## 自由
自由と聞くと、聞こえはいいかもしれないけど、個人が自由に行動できるということは今の社会が許容していない。みんなが満足できる社会を目指している場合、個人の不適切な行動によってそれが阻害されることがあるから。

しかし、それをどこまで許容するかという点ではずっと議論が続いている。それこそ、自殺幇助とか。個人の幸せを達成するために、他人を犠牲にするのは良いのか？

そして、行動の自由だけでなく、情報の自由もずっと議論が続いている。情報は人の行動を大きく変える。人が情報を得る方法も変わってきているし、その得られる情報によって、幸せになる人も、不幸せになる人もいる。最近は情報のアクセスを制限したりして、できるだけ多くの人が幸せになったり、社会全体（国などの単位）をうまくコントロールして目的を達成したりできることを試みる国も少なくない。それに納得する人もいれば、そういった事実を知った人が、怒り狂っているのも見かける。

何が正解なのか自分にもわからないけど、そういったことを頭に入れつつ、生きていこうと思います。