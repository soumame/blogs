---
title: "On LLMs Encouraging Harmful Behavior in People"
emoji: "ğŸ¤–"
tags:
  - "essay"
  - "brain-storming"
  - "llm"
published_at: "2025-12-22T00:00:00.000Z"
description: "I learned that some people claim LLMs are assisting suicide, and now that many people casually use these mysterious black-box LLMs, I want to think about how I should relate to LLMs."
isTranslated: true
sourcePath: "ja/misc/bad-words-from-llms.md"
sourceHash: "e62f521ec2846dee4fd55c05893085dcc6f3e469d2a12f596628b791a9eb730a"
---

I saw an article like this

https://gigazine.net/news/20251108-seven-families-suing-openai-chatgpt-suicides/

Because LLMs learn from human words and actions, by their nature there are cases where they end up encouraging behavior considered harmful to people. It seems there have been incidents where an LLM's response prompted someone to attempt suicide or where a user died as a result. OpenAI is being sued over this.

## Situation in Japan

Well, this is a story from the U.S., and because itâ€™s a litigious society multiple lawsuits may be happening, but I think it could happen in Japan too.

In fact, Japan still has a significant number of suicides, and on social media popular in Japan like X, Iâ€™ve seen people making posts that hint at such behavior.

I think the triggers for suicide are various. I used to think something simple like â€œI wish youâ€™d just dieâ€ would be a trigger, but it seems itâ€™s not that simple.

https://www.mhlw.go.jp/content/001464717.pdf

Not only bullying but also overwork, financial hardship, isolation, and other factors are listed as reasons.

Given such reasons, if someone feels that living is painful, itâ€™s imaginable that some kind of trigger could lead them to attempt suicide.

## Will AI Take Responsibility?

So, what was previously triggered by someoneâ€™s behavior online or by news of a suicide could also plausibly be triggered by statements from an LLM.

Whatâ€™s different from a person is that these systems are toys that mimic human behavior, run by companies on machines.

And those companies cannot take responsibility for everything those machines say. Because they are trained on vast amounts of data from around the world, the creators do not know exactly what data was fed in, and they cannot predict what will come out until it actually does. In short, itâ€™s a black box. I think it already exceeds what a single human can fully understand.

But in the black-box sense, humans are similar: people learn words from others and can output them at appropriate timesâ€”obviously.

Also, itâ€™s the same that we donâ€™t always know what data shaped us. What environment, what community, what background produced a given utterance... a person might remember some of it, but I think most of it is forgotten. By the way, I apparently first pointed at the garbage truck in front of my house and said â€œgo-shi-shi.â€ Thatâ€™s accurate: I put what I saw into words, so itâ€™s the same as a VLM. But why it was a garbage truck, and who I learned it fromâ€”no one knows. It could have been my father, my mother, or a picture book.

The sources of data are unclear, and even if you try to clarify them, in a world overflowing with data itâ€™s extremely difficult to do so.

We humans are placing trust in something (LLMs) whose upbringing we donâ€™t fully understand, and now theyâ€™re everywhere.

Thinking about where responsibility lies is also difficult. For a company, a senior person could simply say theyâ€™ll take responsibility. â€œIâ€™m busy, so Iâ€™ll have the AI do tasks for me. Iâ€™ll take responsibility, and if something goes wrong Iâ€™ll indemnify you,â€ and customers might trust that. In some cases, performance might even exceed a humanâ€™s.

But what if itâ€™s just a conversational companion? Do we put responsibility on our friends? If you told a friend, â€œIf what you say harms me, you must guarantee me,â€ youâ€™d surely lose that friendship. Itâ€™s not like a company-customer relationship where you pay for a product. Of course you share useful information, learn, and enjoy, but you donâ€™t pay, and in most cases (except legally binding relationships like marriage) they donâ€™t take responsibility.

Depending on how LLMs are positioned, for chat-style services that provide text like ChatGPT, Gemini, Claude, I think itâ€™s very hard to set up a mechanism to take that kind of responsibility. If a service promised to provide accurate answers in a particular domain (for example, programming), that might still be possible, but the scope is just too wide. Of course you can put some control in place to prevent inappropriate statements (by not training on certain data or by suppressing outputs when they appear), but if an inappropriate response does slip out, thereâ€™s no end to claims of responsibility. Whatâ€™s more, something that doesnâ€™t look inappropriate might still be inappropriate for a particular person.

### So what should we do

So I think the only practical option is to write a disclaimer and have users accept it when they use the service.

â€œYou can borrow a friend. We raised this child carefully and strictly. We tried to prevent it from learning harmful things. So it probably wonâ€™t say something strange, but it may know things we donâ€™t know or say things that negatively affect you. Do you still want to talk with this person?â€ Thatâ€™s basically all you can ask. It feels odd to equate it to a person.

## Freedom

Freedom sounds nice, but individual freedom of action isnâ€™t fully allowed by todayâ€™s society. If you aim for a society where everyone is satisfied, individual inappropriate actions can interfere with that.

How much of that to permit is an ongoing debate. Thereâ€™s also assisted suicide, and whether sacrificing others to achieve individual happiness is acceptable.

Not only freedom of action but freedom of information has been debated for a long time. Information drastically changes peopleâ€™s behavior. The ways people obtain information are changing, and the information they get can make some people happier and others unhappier. These days, some countries try to restrict access to information to make as many people as possible happy, or to control society (at the level of a nation) to achieve certain goals. Some people accept that, and others, when they learn those facts, become furious.

I donâ€™t know what the right answer is, but Iâ€™ll keep these things in mind as I live on.